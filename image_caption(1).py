# -*- coding: utf-8 -*-
"""Image_caption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bdcrig-BuPdG1zUHpY2lMYsGsGfT79Aa
"""

import pandas as pd
import numpy as np
from pickle import dump
from keras.applications.vgg16 import VGG16
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.models import Model
from os import listdir
import keras
from numpy import array
from pickle import load
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
import string
import os
from keras.preprocessing.text import Tokenizer
from keras.models import load_model

def extract_features(directory):
  model = VGG16()
  model.layers.pop()
  model = Model(inputs=model.inputs, outputs=model.layers[-1].output)
  print(model.summary())
  features = dict()
  if(os.path.isdir(directory)):
    for name in listdir(directory):
      try:
        filename = directory + '/' + name
        image = load_img(filename, target_size=(224, 224))
        image = img_to_array(image)
        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
        image = preprocess_input(image)
        feature = model.predict(image, verbose=0)
        image_id = name.split('.')[0]
        features[image_id] = feature
        print('>%s' % name)
      except Exception as e:
        print(e)
        continue
  else:
    filename = directory
    try:
      image = load_img(filename, target_size=(224, 224))
    except Exception as e:
      print(e)
      
    image = img_to_array(image)
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
    image = preprocess_input(image)
    feature = model.predict(image, verbose=0)
    image_id = filename.split('.')[0]
    features[image_id] = feature
    print('>%s' % filename)
  return features



def load_descriptions(df):
  mapping = dict()
  for index, rows in df.iterrows():
    image_id, image_desc = rows['id'], rows['caption']
    #image_desc = ' '.join(image_desc)
    if image_id not in mapping:
      mapping[image_id] = list()
    mapping[image_id].append(image_desc)
  return mapping

def clean_descriptions(descriptions):
	# prepare translation table for removing punctuation
	table = str.maketrans('', '', string.punctuation)
	for key, desc_list in descriptions.items():
		for i in range(len(desc_list)):
			desc = desc_list[i]
			# tokenize
			desc = desc.split()
			# convert to lower case
			desc = [word.lower() for word in desc]
			# remove punctuation from each token
			desc = [w.translate(table) for w in desc]
			# remove hanging 's' and 'a'
			desc = [word for word in desc if len(word)>1]
			# remove tokens with numbers in them
			desc = [word for word in desc if word.isalpha()]
			# store as string
			desc_list[i] =  ' '.join(desc)

def to_vocabulary(descriptions):
	# build a list of all description strings
	all_desc = set()
	for key in descriptions.keys():
		[all_desc.update(d.split()) for d in descriptions[key]]
	return all_desc

def save_descriptions(descriptions, filename):
	lines = list()
	for key, desc_list in descriptions.items():
		for desc in desc_list:
			lines.append(str(key) + ' ' + desc)
	data = '\n'.join(lines)
	file = open(filename, 'w')
	file.write(data)
	file.close()

def load_doc(filename):
	# open the file as read only
	file = open(filename, 'r')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text
 
# load a pre-defined list of photo identifiers
def load_set(df):
	#doc = load_doc(filename)
	dataset = list()
	
	for index, rows in df.iterrows():
		# skip empty lines
		
		# get the image identifier
		identifier = rows['id']
		dataset.append(identifier)
	return set(dataset)

def load_clean_descriptions(filename, dataset):
	
	doc = load_doc(filename)
	descriptions = dict()
	for line in doc.split('\n'):
		
		tokens = line.split()
      
		
		image_id, image_desc = tokens[0], tokens[1:]
		
		if image_id in dataset:
			# create list
			if image_id not in descriptions:
				descriptions[image_id] = list()
			# wrap description in tokens
			desc = 'startseq ' + ' '.join(image_desc) + ' endseq'
			# store
			descriptions[image_id].append(desc)
	return descriptions

def load_clean(df_new, dataset):
	
	
	descriptions = dict()
	for index, rows in df_new.iterrows():
		
		
      
		
		image_id, image_desc = rows['id'], rows['caption']
		
		if image_id in dataset:
			# create list
			if image_id not in descriptions:
				descriptions[image_id] = list()
			# wrap description in tokens
			desc = 'startseq ' + ' '.join(image_desc) + ' endseq'
			# store
			descriptions[image_id].append(desc)
	return descriptions


def load_photo_features(filename, dataset):
  all_features = load(open(filename, 'rb'))
  features = {}
  print(all_features)
  for k in dataset:
    #print(k)
    s = str(k)
    #print(s)
    #print(all_features[s])
    try:
      features[k] = all_features[s]
    except:
      #print('error')
      continue
  return features

def load_photo_features_test(filename, dataset):
  all_features = load(open(filename, 'rb'))
  features = {}
  print(all_features)
  for k in dataset:
    #print(k)
    s = str(k)
    #print(s)
    #print(all_features[s])
    try:
      features[k] = all_features[s]
    except:
      #print('error')
      continue
  return features


def to_lines(descriptions):
	all_desc = list()
	for key in descriptions.keys():
		[all_desc.append(d) for d in descriptions[key]]
	return all_desc
 

def create_tokenizer(descriptions):
	lines = to_lines(descriptions)
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts(lines)
	return tokenizer

def data_generator(descriptions, photos, tokenizer, max_length):
  while 1:
    for key, desc_list in descriptions.items():
      try:
        photo = photos[key][0]
        in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo);
        yield [[in_img, in_seq], out_word];
      except:
        continue

def create_sequences(tokenizer, max_length, descriptions, photos):
  X1, X2, y = list(), list(), list()
  for key, desc_list in descriptions.items():
    for desc in desc_list:
      seq = tokenizer.texts_to_sequences([desc])[0]
      for i in range(1, len(seq)):
        in_seq, out_seq = seq[:i], seq[i]
        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
        try:
          X1.append(photos[key][0])
          X2.append(in_seq)
          y.append(out_seq)
        except:
          continue
  return array(X1), array(X2), array(y)

def create_sequences(tokenizer, max_length, desc_list, photo):
  X1, X2, y = list(), list(), list()
  for desc in desc_list:
    #print(tokenizer.texts_to_sequences([desc])[0])
    seq = tokenizer.texts_to_sequences([desc])[0]
    for i in range(1, len(seq)):
      in_seq, out_seq = seq[:i], seq[i]
      in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
      out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
      try:
        X1.append(photo)
        X2.append(in_seq)
        y.append(out_seq)
      except:
        continue
  return array(X1), array(X2), array(y)


def max_length(descriptions):
	lines = to_lines(descriptions)
	return max(len(d.split()) for d in lines)

def define_model(vocab_size, max_length):
	# feature extractor model
	inputs1 = Input(shape=(4096,))
	fe1 = Dropout(0.5)(inputs1)
	fe2 = Dense(256, activation='relu')(fe1)
	# sequence model
	inputs2 = Input(shape=(max_length,))
	se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
	se2 = Dropout(0.5)(se1)
	se3 = LSTM(256)(se2)
	# decoder model
	decoder1 = add([fe2, se3])
	decoder2 = Dense(256, activation='relu')(decoder1)
	outputs = Dense(vocab_size, activation='softmax')(decoder2)
	# tie it together [image, seq] [word]
	model = Model(inputs=[inputs1, inputs2], outputs=outputs)
	model.compile(loss='categorical_crossentropy', optimizer='adam')
	# summarize model
	print(model.summary())
	plot_model(model, to_file='model.png', show_shapes=True)
	return model

def word_for_id(integer, tokenizer):
	for word, index in tokenizer.word_index.items():
		if index == integer:
			return word
	return None
 
# generate a description for an image
def generate_desc(model, tokenizer, photo, max_length):
	
	in_text = 'startseq'
	# iterate over the whole length of the sequence
	for i in range(max_length):
		# integer encode input sequence
		sequence = tokenizer.texts_to_sequences([in_text])[0]
		# pad input
		sequence = pad_sequences([sequence], maxlen=max_length)
		# predict next word
		yhat = model.predict([photo,sequence], verbose=0)
		# convert probability to integer
		yhat = argmax(yhat)
		# map integer to word
		word = word_for_id(yhat, tokenizer)
		# stop if we cannot map the word
		if word is None:
			break
		# append as input for generating the next word
		in_text += ' ' + word
		# stop if we predict the end of the sequence
		if word == 'endseq':
			break
	return in_text

def evaluate_model(model, descriptions, photos, tokenizer, max_length):
	actual, predicted = list(), list()
	# step over the whole set
	for key, desc_list in descriptions.items():
		# generate description
		yhat = generate_desc(model, tokenizer, photos[key], max_length)
		# store actual and predicted
		references = [d.split() for d in desc_list]
		actual.append(references)
		predicted.append(yhat.split())
	# calculate BLEU score
	print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))
	print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))
	print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))
	print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))


#--------------------------------------------------------------------------
dir = 'Directory name of data set'
df_cap = pd.read_csv('Address of csv file for captions')
features = extract_features(dir)
print('Extracted Features: %d' % len(features))
# save to file
dump(features, open('features.pkl', 'wb'))

descriptions = load_descriptions(df_cap)
print('Loaded: %d ' % len(descriptions))

clean_descriptions(descriptions)

vocabulary = to_vocabulary(descriptions)
print('Vocabulary Size: %d' % len(vocabulary))

l1 = []
l2 = []
for key, value in descriptions.items():
  l1.append(key)
  l2.append(value[0])
d = {'id':l1, 'caption':l2}
new_df = pd.DataFrame(data = d)
new_df.head()

save_descriptions(descriptions, 'descriptions.txt')

train = load_set(df_cap)
print('Dataset: %d' % len(train))

train_descriptions = load_clean(new_df, train)
print('Descriptions: train=%d' % len(train_descriptions))

train_features = load_photo_features('features.pkl', train)
print('Photos: train=%d' % len(train_features))

tokenizer = create_tokenizer(train_descriptions)
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary Size: %d' % vocab_size)

max_len = max_length(descriptions)
model = define_model(vocab_size, max_len)


generator = data_generator(train_descriptions, train_features, tokenizer, max_len)
inputs, outputs = next(generator)
print(inputs[0].shape)
print(inputs[1].shape)
print(outputs.shape)

steps = len(train_descriptions)
epochs = 20
for i in range(epochs):
	# create the data generator
	generator = data_generator(train_descriptions, train_features, tokenizer, max_len);
	
	model.fit_generator(generator, epochs, steps_per_epoch=steps, verbose=2);
	
	model.save('model_' + str(i) + '.h5')


df_cap_test = pd.read_csv('Address for test captions csv file')
test = load_set(df_cap_test)
print('Dataset: %d' % len(test))
test_descriptions = load_clean(new_df, test)
print(test)
print(len(test))

test_features = load_photo_features('features.pkl', test)
print('Photos: test=%d' % len(test_features))

directory = 'address for test set images'
features_test = extract_features_test(directory)
print('Extracted Features: %d' % len(features_test))
dump(features, open('features_test.pkl', 'wb'))

tokenizer = create_tokenizer(train_descriptions)
dump(tokenizer, open('tokenizer.pkl', 'wb'))

tokenizer = load(open('tokenizer.pkl', 'rb'))
max_length = 34
model = load_model('model_1.h5') # str(i) = 1

photo = extract_features('dog.jpg') # give a test image
#print(photo)
description = generate_desc(model, tokenizer, photo, 34)
print(description)